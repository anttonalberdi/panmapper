import os
import pandas as pd
from glob import glob

# Configurations
configfile: "config/config.yaml"

# Database directories
CHECKM2_DATABASE = config["checkm2_database"]
GTDB_DATABASE = config["gtdb_database"]

# Input and output directories
GENOME_DIR = config["genome_dir"]
READS_DIR = config["reads_dir"]
TEMP_DIR = config["temp_dir"]
OUTPUT_DIR = config["output_dir"]

# Output subdirectories
DREP_DIR = f"{OUTPUT_DIR}/drep"
PRODIGAL_DIR = f"{OUTPUT_DIR}/prodigal"
CLUSTER_DIR = f"{OUTPUT_DIR}/clusters"
MMSEQS_DIR = f"{OUTPUT_DIR}/mmseqs"
CHECKM2_DIR = f"{OUTPUT_DIR}/checkm2"
GTDBTK_DIR = f"{OUTPUT_DIR}/gtdbtk"
KOFAMS_DIR = f"{OUTPUT_DIR}/kofams"
BOWTIE_DIR = f"{OUTPUT_DIR}/bowtie"
COVERM_DIR = f"{OUTPUT_DIR}/coverm"

# Infer genomes and samples
genomes, = glob_wildcards(f"{GENOME_DIR}/{{genome}}.fna")
samples, = glob_wildcards(f"{READS_DIR}/{{sample}}_1.fq.gz")


#Read the primary_cluster column from the Cdb.csv file produced by drep and return unique clusters
def get_unique_clusters(cdb_file):
    if not os.path.exists(cdb_file):
        return []  # Return an empty list if the file doesn't exist yet
    df = pd.read_csv(cdb_file)
    return df["primary_cluster"].unique().tolist()

# Rules
rule all:
    input:
        expand(f"{GENOME_DIR}/{{genome}}.fna", genome=genomes),
        f"{DREP_DIR}",
        f"{BOWTIE_DIR}/all_clusters.fna",
        f"{CHECKM2_DIR}",
        f"{GTDBTK_DIR}",
        expand(f"{BOWTIE_DIR}/all_clusters.1.bt2", ext=["1.bt2", "2.bt2", "3.bt2", "4.bt2", "rev.1.bt2", "rev.2.bt2"]),
        expand(f"{BOWTIE_DIR}/{{sample}}.bam", sample=samples),
        f"{COVERM_DIR}/all_clusters.tsv"

rule run_drep:
    input:
        genomes=expand(f"{GENOME_DIR}/{{genome}}.fna", genome=genomes)
    output:
        dir=directory(f"{DREP_DIR}"),
        csv=f"{DREP_DIR}/data_tables/Cdb.csv"
    conda:
        "environments/clustering.yml"
    shell:
        """
        dRep compare {output.dir} -g {input}
        """

rule run_prodigal:
    input:
        f"{GENOME_DIR}/{{genome}}.fna"
    output:
        f"{PRODIGAL_DIR}/{{genome}}.fna"
    conda:
        "environments/clustering.yml"
    shell:
        """
        prodigal -i {input} -d {output}
        """

# Precompute unique_clusters based on the Cdb.csv file
if os.path.exists(f"{DREP_DIR}/data_tables/Cdb.csv"):
    cdb_df = pd.read_csv(f"{DREP_DIR}/data_tables/Cdb.csv")
    unique_clusters = cdb_df["primary_cluster"].unique()
else:
    unique_clusters = []

rule cluster_genomes:
    input:
        csv=f"{DREP_DIR}/data_tables/Cdb.csv",
        prodigal_files=expand(f"{PRODIGAL_DIR}/{{genome}}.fna", genome=genomes)
    output:
        expand(f"{CLUSTER_DIR}/cluster_{{cluster_id}}.fna", cluster_id=unique_clusters),
        touch(f"{CLUSTER_DIR}/done.txt")
    run:
        # Read clustering information from the CSV
        df = pd.read_csv(input.csv)
        cluster_dict = df.groupby("primary_cluster")["genome"].apply(list).to_dict()

        os.makedirs(CLUSTER_DIR, exist_ok=True)

        # Create cluster files by concatenating genomes
        for cluster_id, genome_list in cluster_dict.items():
            cluster_file = f"{CLUSTER_DIR}/cluster_{cluster_id}.fna"
            with open(cluster_file, "w") as out_file:
                for genome_id in genome_list:
                    prodigal_file = f"{PRODIGAL_DIR}/{genome_id}"
                    if prodigal_file in input.prodigal_files:
                        with open(prodigal_file, "r") as f:
                            out_file.write(f.read())

rule run_mmseqs:
    input:
        cluster_genes=f"{CLUSTER_DIR}/cluster_{{cluster_id}}.fna"
    output:
        rep_fna=f"{MMSEQS_DIR}/cluster_{{cluster_id}}_genes_drep.fna",
        rep_tsv=f"{MMSEQS_DIR}/cluster_{{cluster_id}}_genes_drep.tsv"
    params:
        tmp=f"{MMSEQS_DIR}/cluster_{{cluster_id}}_tmp"
    conda:
        "environments/clustering.yml"
    shell:
        """
        mmseqs easy-linclust {input.cluster_genes} {MMSEQS_DIR}/cluster_{wildcards.cluster_id}_output {params.tmp} --min-seq-id 0.8 --cov-mode 1 -c 0.8
        rm {MMSEQS_DIR}/cluster_{wildcards.cluster_id}_output_all_seqs.fasta
        mv {MMSEQS_DIR}/cluster_{wildcards.cluster_id}_output_rep_seq.fasta {output.rep_fna}
        mv {MMSEQS_DIR}/cluster_{wildcards.cluster_id}_output_cluster.tsv {output.rep_tsv}
        rm -rf {params.tmp}
        """

rule run_checkm2:
    input:
        clusters=expand(f"{CLUSTER_DIR}/cluster_{{cluster_id}}.fna", cluster_id=unique_clusters),
        placeholder=f"{CLUSTER_DIR}/done.txt"
    output:
        directory(f"{CHECKM2_DIR}")
    params:
        database=f"{CHECKM2_DATABASE}"
    conda:
        "environments/quantification.yml"
    shell:
        """
        checkm2 predict --threads {threads} --input {input.clusters} --output-directory {output} --database_path {params.database}
        """

rule run_gtdbtk:
    input:
        clusters=expand(f"{CLUSTER_DIR}/cluster_{{cluster_id}}.fna", cluster_id=unique_clusters),
        placeholder=f"{CLUSTER_DIR}/done.txt"
    output:
        directory(f"{GTDBTK_DIR}")
    params:
        inputdir=f"{CLUSTER_DIR}",
        database=f"{GTDB_DATABASE}",
        tmp=f"{TEMP_DIR}"
    conda:
        "environments/quantification.yml"
    shell:
        """
        export GTDBTK_DATA_PATH={params.database}
        mkdir {output}
        gtdbtk classify_wf \
            --genome_dir {params.inputdir} \
            --extension fna \
            --out_dir {output} \
            --tmpdir {params.tmp} \
            --cpus {threads} \
            --skip_ani_screen
        """

#rule run_kofams:

rule concatenate_clusters:
    input:
        clusters=expand(f"{CLUSTER_DIR}/cluster_{{cluster_id}}.fna", cluster_id=unique_clusters),
        placeholder=f"{CLUSTER_DIR}/done.txt"
    output:
        f"{BOWTIE_DIR}/all_clusters.fna"
    run:
        with open(output[0], "w") as out_file:
            for cluster_file in input.clusters:
                with open(cluster_file, "r") as in_file:
                    out_file.write(in_file.read())

rule build_bowtie_index:
    input:
        f"{BOWTIE_DIR}/all_clusters.fna"
    output:
        expand(f"{BOWTIE_DIR}/all_clusters.{{ext}}", ext=["1.bt2", "2.bt2", "3.bt2", "4.bt2", "rev.1.bt2", "rev.2.bt2"])
    conda:
        "environments/quantification.yml"
    shell:
        """
        mkdir -p {BOWTIE_DIR}
        bowtie2-build {input} {BOWTIE_DIR}/all_clusters
        """

rule map_reads:
    input:
        index=expand(f"{BOWTIE_DIR}/all_clusters.{{ext}}", ext=["1.bt2", "2.bt2", "3.bt2", "4.bt2", "rev.1.bt2", "rev.2.bt2"]),
        r1=f"{READS_DIR}/{{sample}}_1.fq.gz",
        r2=f"{READS_DIR}/{{sample}}_2.fq.gz"
    output:
        f"{BOWTIE_DIR}/{{sample}}.bam"
    conda:
        "environments/quantification.yml"
    shell:
        """
        bowtie2 -x {BOWTIE_DIR}/all_clusters -1 {input.r1} -2 {input.r2} | samtools view -bS - | samtools sort -o {output}
        """

rule quantify_reads:
    input:
        expand(f"{BOWTIE_DIR}/{{sample}}.bam", sample=samples)
    output:
        f"{COVERM_DIR}/all_clusters.tsv"
    conda:
        "environments/quantification.yml"
    shell:
        """
        coverm contig -m count covered_bases -b {input} -t {threads} -o {output}
        """
