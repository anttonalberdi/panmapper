import os
import pandas as pd
from glob import glob

# Configurations
configfile: "config/config.yaml"

# Database directories
CHECKM2_DATABASE = config["checkm2_database"]
GTDB_DATABASE = config["gtdb_database"]
KOFAMS_DATABASE = config["kofams_database"]

# Input and output directories
GENOME_DIR = config["genome_dir"]
READS_DIR = config["reads_dir"]
TEMP_DIR = config["temp_dir"]
OUTPUT_DIR = config["output_dir"]

# Output subdirectories
DREP_DIR = f"{OUTPUT_DIR}/drep"
PRODIGAL_DIR = f"{OUTPUT_DIR}/prodigal"
CLUSTER_DIR = f"{OUTPUT_DIR}/clusters"
MMSEQS_DIR = f"{OUTPUT_DIR}/mmseqs"
CHECKM2_DIR = f"{OUTPUT_DIR}/checkm2"
GTDBTK_DIR = f"{OUTPUT_DIR}/gtdbtk"
KOFAMS_DIR = f"{OUTPUT_DIR}/kofams"
BOWTIE_DIR = f"{OUTPUT_DIR}/bowtie"
COVERM_DIR = f"{OUTPUT_DIR}/coverm"
FINAL_DIR = f"{OUTPUT_DIR}/final"

# Infer genomes and samples
genomes, = glob_wildcards(f"{GENOME_DIR}/{{genome}}.fna")
samples, = glob_wildcards(f"{READS_DIR}/{{sample}}_1.fq.gz")

#Read the secondary_cluster column from the Cdb.csv file produced by drep and return unique clusters
def get_unique_clusters(cdb_file):
    if not os.path.exists(cdb_file):
        return []  # Return an empty list if the file doesn't exist yet
    df = pd.read_csv(cdb_file)
    return df["secondary_cluster"].unique().tolist()

# Rules
rule all:
    input:
        expand(f"{GENOME_DIR}/{{genome}}.fna", genome=genomes),
        f"{FINAL_DIR}/genome_contig.csv",
        f"{DREP_DIR}",
        f"{BOWTIE_DIR}/all_clusters.fna",
        #f"{CHECKM2_DIR}",
        #f"{GTDBTK_DIR}",
        f"{KOFAMS_DIR}/all_clusters.faa",
        #expand(f"{BOWTIE_DIR}/all_clusters.1.bt2", ext=["1.bt2", "2.bt2", "3.bt2", "4.bt2", "rev.1.bt2", "rev.2.bt2"]),
        #expand(f"{BOWTIE_DIR}/{{sample}}.bam", sample=samples),
        f"{FINAL_DIR}/cluster_info.csv",
        f"{FINAL_DIR}/counts.csv",
        f"{FINAL_DIR}/coverage.csv"

rule run_drep:
    input:
        genomes=expand(f"{GENOME_DIR}/{{genome}}.fna", genome=genomes)
    output:
        dir=directory(f"{DREP_DIR}"),
        csv=f"{DREP_DIR}/data_tables/Cdb.csv"
    conda:
        "environments/clustering.yml"
    shell:
        """
        dRep compare {output.dir} -g {input}
        """

rule run_prodigal:
    input:
        f"{GENOME_DIR}/{{genome}}.fna"
    output:
        nt=f"{PRODIGAL_DIR}/{{genome}}.fna",
        aa=f"{PRODIGAL_DIR}/{{genome}}.faa"
    conda:
        "environments/clustering.yml"
    shell:
        """
        prodigal -i {input} -d {output.nt} -a {output.aa}
        """

rule create_genome_gene_table:
    input:
        expand(f"{PRODIGAL_DIR}/{{genome}}.fna", genome=genomes)
    output:
        f"{FINAL_DIR}/genome_gene.csv"
    shell:
        """
        python workflow/scripts/genome_gene_table.py {input} {output}
        """

# Precompute unique_clusters based on the Cdb.csv file
if os.path.exists(f"{DREP_DIR}/data_tables/Cdb.csv"):
    cdb_df = pd.read_csv(f"{DREP_DIR}/data_tables/Cdb.csv")
    unique_clusters = cdb_df["secondary_cluster"].unique()
else:
    unique_clusters = []

rule cluster_genomes:
    input:
        csv=f"{DREP_DIR}/data_tables/Cdb.csv",
        nt=expand(f"{PRODIGAL_DIR}/{{genome}}.fna", genome=genomes),
        aa=expand(f"{PRODIGAL_DIR}/{{genome}}.faa", genome=genomes)
    output:
        expand(f"{CLUSTER_DIR}/cluster_{{cluster_id}}.fna", cluster_id=unique_clusters),
        expand(f"{CLUSTER_DIR}/cluster_{{cluster_id}}.faa", cluster_id=unique_clusters),
        touch(f"{CLUSTER_DIR}/done.txt")
    run:
        # Read clustering information from the CSV
        df = pd.read_csv(input.csv)
        cluster_dict = df.groupby("secondary_cluster")["genome"].apply(list).to_dict()

        os.makedirs(CLUSTER_DIR, exist_ok=True)

        # Create cluster files by concatenating genomes
        for cluster_id, genome_list in cluster_dict.items():
            #Generate nucleotide sequences
            cluster_file = f"{CLUSTER_DIR}/cluster_{cluster_id}.fna"
            with open(cluster_file, "w") as out_file:
                for genome_id in genome_list:
                    prodigal_file = f"{PRODIGAL_DIR}/{genome_id}"
                    if prodigal_file in input.nt:
                        with open(prodigal_file, "r") as f:
                            out_file.write(f.read())
            #Generate amino acid sequences
            cluster_file = f"{CLUSTER_DIR}/cluster_{cluster_id}.faa"
            with open(cluster_file, "w") as out_file:
                for genome_id in genome_list:
                    genome_id2 = genome_id.replace(".fna", ".faa")
                    prodigal_file2 = f"{PRODIGAL_DIR}/{genome_id2}"
                    if prodigal_file2 in input.aa:
                        with open(prodigal_file2, "r") as f:
                            out_file.write(f.read())
rule run_mmseqs:
    input:
        cluster_genes=f"{CLUSTER_DIR}/cluster_{{cluster_id}}.fna",
        placeholder=f"{CLUSTER_DIR}/done.txt"
    output:
        fna=f"{MMSEQS_DIR}/cluster_{{cluster_id}}.fna",
        tsv=f"{MMSEQS_DIR}/cluster_{{cluster_id}}.tsv"
    params:
        tmp=f"{MMSEQS_DIR}/cluster_{{cluster_id}}_tmp"
    conda:
        "environments/clustering.yml"
    shell:
        """
        mmseqs easy-linclust {input.cluster_genes} {MMSEQS_DIR}/cluster_{wildcards.cluster_id}_output {params.tmp} --min-seq-id 0.8 --cov-mode 1 -c 0.8
        rm {MMSEQS_DIR}/cluster_{wildcards.cluster_id}_output_all_seqs.fasta
        mv {MMSEQS_DIR}/cluster_{wildcards.cluster_id}_output_rep_seq.fasta {output.fna}
        mv {MMSEQS_DIR}/cluster_{wildcards.cluster_id}_output_cluster.tsv {output.tsv}
        rm -rf {params.tmp}
        """


rule translate_genes:
    input:
        f"{MMSEQS_DIR}/cluster_{{cluster_id}}.fna"
    output:
        f"{MMSEQS_DIR}/cluster_{{cluster_id}}.faa"
    shell:
        """
        python workflow/scripts/translate_genes.py {input} {output}
        """

rule run_checkm2:
    input:
        clusters=expand(f"{MMSEQS_DIR}/cluster_{{cluster_id}}.fna", cluster_id=unique_clusters)
    output:
        directory(f"{CHECKM2_DIR}")
    params:
        database=f"{CHECKM2_DATABASE}"
    conda:
        "environments/quantification.yml"
    shell:
        """
        checkm2 predict --threads {threads} --input {input.clusters} --output-directory {output} --database_path {params.database}
        """

rule run_gtdbtk:
    input:
        clusters=expand(f"{MMSEQS_DIR}/cluster_{{cluster_id}}.fna", cluster_id=unique_clusters)
    output:
        directory(f"{GTDBTK_DIR}")
    params:
        inputdir=f"{CLUSTER_DIR}",
        database=f"{GTDB_DATABASE}",
        tmp=f"{TEMP_DIR}"
    conda:
        "environments/quantification.yml"
    shell:
        """
        export GTDBTK_DATA_PATH={params.database}
        mkdir {output}
        gtdbtk classify_wf \
            --genome_dir {params.inputdir} \
            --extension fna \
            --out_dir {output} \
            --tmpdir {params.tmp} \
            --cpus {threads} \
            --skip_ani_screen
        """

rule concatenate_clusters_nt:
    input:
        clusters=expand(f"{MMSEQS_DIR}/cluster_{{cluster_id}}.fna", cluster_id=unique_clusters)
    output:
        f"{BOWTIE_DIR}/all_clusters.fna"
    run:
        with open(output[0], "w") as out_file:
            for cluster_file in input.clusters:
                with open(cluster_file, "r") as in_file:
                    out_file.write(in_file.read())

rule concatenate_clusters_aa:
    input:
        clusters=expand(f"{MMSEQS_DIR}/cluster_{{cluster_id}}.faa", cluster_id=unique_clusters)
    output:
        f"{KOFAMS_DIR}/all_clusters.faa"
    run:
        with open(output[0], "w") as out_file:
            for cluster_file in input.clusters:
                with open(cluster_file, "r") as in_file:
                    out_file.write(in_file.read())

rule run_kofams:
    input:
        aa=f"{MMSEQS_DIR}/cluster_{{cluster_id}}.faa"
    output:
        txt=f"{KOFAMS_DIR}/cluster_{{cluster_id}}.txt",
        tsv=f"{KOFAMS_DIR}/cluster_{{cluster_id}}.tsv"
    params:
        database=f"{KOFAMS_DATABASE}"
    conda:
        "environments/quantification.yml"
    shell:
        """
        hmmscan -o {output.txt} --tblout {output.tsv} -E 1e-5 --noali {params.database} {input.aa}
        """

rule merge_kofams:
    input:
        expand(f"{KOFAMS_DIR}/cluster_{{cluster_id}}.tsv", cluster_id=unique_clusters)
    output:
        tsv=f"{KOFAMS_DIR}/all_clusters.tsv",
        csv=f"{KOFAMS_DIR}/all_clusters.csv"
    params:
        database=f"{KOFAMS_DATABASE}"
    conda:
        "environments/quantification.yml"
    shell:
        """
        cat {input} > {output.tsv}
        python workflow/scripts/select_ko.py {output.tsv} {output.csv}
        """

rule build_bowtie_index:
    input:
        f"{BOWTIE_DIR}/all_clusters.fna"
    output:
        index=expand(f"{BOWTIE_DIR}/all_clusters.{{ext}}", ext=["1.bt2", "2.bt2", "3.bt2", "4.bt2", "rev.1.bt2", "rev.2.bt2"]),
        lengths=f"{FINAL_DIR}/gene_lengths.csv"
    conda:
        "environments/quantification.yml"
    shell:
        """
        mkdir -p {BOWTIE_DIR}
        bowtie2-build {input} {BOWTIE_DIR}/all_clusters
        python workflow/scripts/calculate_gene_lengths.py {input} {output.lengths}
        """

rule map_reads:
    input:
        index=expand(f"{BOWTIE_DIR}/all_clusters.{{ext}}", ext=["1.bt2", "2.bt2", "3.bt2", "4.bt2", "rev.1.bt2", "rev.2.bt2"]),
        r1=f"{READS_DIR}/{{sample}}_1.fq.gz",
        r2=f"{READS_DIR}/{{sample}}_2.fq.gz"
    output:
        f"{BOWTIE_DIR}/{{sample}}.bam"
    conda:
        "environments/quantification.yml"
    shell:
        """
        bowtie2 -x {BOWTIE_DIR}/all_clusters -1 {input.r1} -2 {input.r2} | samtools view -bS - | samtools sort -o {output}
        """

rule quantify_reads:
    input:
        expand(f"{BOWTIE_DIR}/{{sample}}.bam", sample=samples)
    output:
        f"{COVERM_DIR}/all_clusters.tsv"
    conda:
        "environments/quantification.yml"
    shell:
        """
        coverm contig -m count covered_bases -b {input} -t {threads} -o {output}
        """

rule final_files:
    input:
        genomes=f"{FINAL_DIR}/genome_gene.csv",
        lengths=f"{FINAL_DIR}/contig_lengths.csv",
        coverm=f"{COVERM_DIR}/all_clusters.tsv",
        clusters=f"{DREP_DIR}/data_tables/Cdb.csv",
        kofams=f"{KOFAMS_DIR}/all_clusters.csv"
    output:
        info=f"{FINAL_DIR}/cluster_info.csv",
        counts=f"{FINAL_DIR}/counts.csv",
        coverage=f"{FINAL_DIR}/coverage.csv"
    conda:
        "environments/quantification.yml"
    shell:
        """
        python workflow/scripts/separate_coverm.py {input.coverm} {output.counts} {output.coverage}
        python workflow/scripts/pangenome_info.py {input.clusters} {input.genomes} {input.lengths} {input.kofams} {output.info}
        """
