#### Load python modules

import os
import pandas as pd
from glob import glob

#### Configuration

configfile: "config/config.yaml"

# Database directories
CHECKM2_DATABASE = config["checkm2_database"]
GTDB_DATABASE = config["gtdb_database"]
KOFAMS_DATABASE = config["kofams_database"]

# Input and output directories
GENOME_DIR = config["genome_dir"]
READS_DIR = config["reads_dir"]
TEMP_DIR = config["temp_dir"]
OUTPUT_DIR = config["output_dir"]

# Output subdirectories
DREP_DIR = f"{OUTPUT_DIR}/drep"
PRODIGAL_DIR = f"{OUTPUT_DIR}/prodigal"
CLUSTER_DIR = f"{OUTPUT_DIR}/clusters"
MMSEQS_DIR = f"{OUTPUT_DIR}/mmseqs"
CHECKM2_GENOMES_DIR = f"{OUTPUT_DIR}/checkm2_genomes"
CHECKM2_CLUSTERS_DIR = f"{OUTPUT_DIR}/checkm2_clusters"
GTDBTK_DIR = f"{OUTPUT_DIR}/gtdbtk"
KOFAMS_DIR = f"{OUTPUT_DIR}/kofams"
BOWTIE_DIR = f"{OUTPUT_DIR}/bowtie"
COVERM_DIR = f"{OUTPUT_DIR}/coverm"
FIGURES_DIR = f"{OUTPUT_DIR}/figures"
FINAL_DIR = f"{OUTPUT_DIR}/final"

# Infer genomes and samples
genomes, = glob_wildcards(f"{GENOME_DIR}/{{genome}}.fna")
samples, = glob_wildcards(f"{READS_DIR}/{{sample}}_1.fq.gz")

####  Rules

# Final files that need to be generated
rule all:
    input:
        f"{CHECKM2_GENOMES_DIR}/quality_report.tsv",
        f"{CHECKM2_CLUSTERS_DIR}/quality_report.tsv",
        f"{GTDBTK_DIR}",
        f"{FINAL_DIR}/cluster_info.csv",
        f"{FINAL_DIR}/gene_counts.csv",
        f"{FINAL_DIR}/gene_coverage.csv",
        f"{FINAL_DIR}/cluster_counts.csv",
        f"{FINAL_DIR}/cluster_coverage.csv",
        f"{FIGURES_DIR}/heatmap_genes.pdf",
        f"{FIGURES_DIR}/heatmap_counts.pdf"
        #f"{FIGURES_DIR}/heatmap_kegg.pdf" # not working

# Predict genes from all original genomes
rule run_prodigal:
    input:
        f"{GENOME_DIR}/{{genome}}.fna"
    output:
        nt=f"{PRODIGAL_DIR}/{{genome}}.fna",
        aa=f"{PRODIGAL_DIR}/{{genome}}.faa"
    conda:
        "environments/clustering.yml"
    shell:
        """
        prodigal -i {input} -d {output.nt} -a {output.aa}
        """

# Run checkm2 in the original genomes
rule run_checkm2_genomes:
    input:
        expand(f"{GENOME_DIR}/{{genome}}.fna", genome=genomes)
    output:
        directory=directory(f"{CHECKM2_GENOMES_DIR}"),
        report=f"{CHECKM2_GENOMES_DIR}/quality_report.tsv"
    params:
        database=f"{CHECKM2_DATABASE}"
    conda:
        "environments/quantification.yml"
    shell:
        """
        checkm2 predict --threads {threads} --input {input} --output-directory {output.directory} --database_path {params.database}
        """

# Calculate ANI values to generate clusters. Clusters are outputed in the Cdb.csv file.
rule run_drep:
    input:
        genomes=expand(f"{GENOME_DIR}/{{genome}}.fna", genome=genomes)
    output:
        dir=directory(f"{DREP_DIR}"),
        csv=f"{DREP_DIR}/data_tables/Cdb.csv"
    conda:
        "environments/clustering.yml"
    shell:
        """
        dRep compare {output.dir} -g {input}
        """

# Create a reference table with genome and gene names to trace orgigin of genes
rule create_genome_gene_table:
    input:
        expand(f"{PRODIGAL_DIR}/{{genome}}.fna", genome=genomes)
    output:
        f"{FINAL_DIR}/genome_gene.csv"
    shell:
        """
        python workflow/scripts/genome_gene_table.py {input} {output}
        """

# Cluster original genomes into group of genomes to be dereplicated using mmseqs
# Note this this a checkpoint. The outputs of this rule cannot be predicted, as
# they depend on the content of the Cdb.csv file generated in the previous drep rule.
# In consequence, the checkpoint forces snakemake to re-create the pipeline structure
# and consider the new wildards. Note the use of functions to generate the dynamic
# inputs for the direct dependencies of this rule in the subsequent steps.

checkpoint cluster_genomes:
    input:
        csv=f"{DREP_DIR}/data_tables/Cdb.csv",
        nt=expand(f"{PRODIGAL_DIR}/{{genome}}.fna", genome=genomes),
        aa=expand(f"{PRODIGAL_DIR}/{{genome}}.faa", genome=genomes)
    output:
        directory(CLUSTER_DIR)
    run:
        import pandas as pd
        import os

        # Read clustering information from the CSV
        df = pd.read_csv(input.csv)
        cluster_dict = df.groupby("secondary_cluster")["genome"].apply(list).to_dict()

        os.makedirs(CLUSTER_DIR, exist_ok=True)

        # Create cluster files by concatenating genomes
        for cluster_id, genome_list in cluster_dict.items():
            # Generate nucleotide sequences
            cluster_file_nt = f"{CLUSTER_DIR}/cluster_{cluster_id}.fna"
            with open(cluster_file_nt, "w") as out_file:
                for genome_id in genome_list:
                    prodigal_file = f"{PRODIGAL_DIR}/{genome_id}"
                    if prodigal_file in input.nt:
                        with open(prodigal_file, "r") as f:
                            out_file.write(f.read())
            # Generate amino acid sequences
            cluster_file_aa = f"{CLUSTER_DIR}/cluster_{cluster_id}.faa"
            with open(cluster_file_aa, "w") as out_file:
                for genome_id in genome_list:
                    genome_id_aa = genome_id.replace(".fna", ".faa")
                    prodigal_file_aa = f"{PRODIGAL_DIR}/{genome_id_aa}"
                    if prodigal_file_aa in input.aa:
                        with open(prodigal_file_aa, "r") as f:
                            out_file.write(f.read())

# Functions to define the input files dynamically.

def get_cluster_ids_from_csv(csv_path):
    df = pd.read_csv(csv_path)
    return df["secondary_cluster"].unique()

## These functions return each file separately, and need to be combined with a lambda expression in the input

def get_cluster_fna_sep(wildcards):
    checkpoint_output = checkpoints.cluster_genomes.get(**wildcards).output[0]
    cluster_ids = get_cluster_ids_from_csv(f"{DREP_DIR}/data_tables/Cdb.csv")
    return f"{CLUSTER_DIR}/cluster_{wildcards.cluster_id}.fna"

def get_gene_fna_sep(wildcards):
    checkpoint_output = checkpoints.cluster_genomes.get(**wildcards).output[0]
    cluster_ids = get_cluster_ids_from_csv(f"{DREP_DIR}/data_tables/Cdb.csv")
    return f"{MMSEQS_DIR}/cluster_{wildcards.cluster_id}.fna"

def get_gene_faa_sep(wildcards):
    checkpoint_output = checkpoints.cluster_genomes.get(**wildcards).output[0]
    cluster_ids = get_cluster_ids_from_csv(f"{DREP_DIR}/data_tables/Cdb.csv")
    return f"{MMSEQS_DIR}/cluster_{wildcards.cluster_id}.faa"

def get_gene_tsv_sep(wildcards):
    checkpoint_output = checkpoints.cluster_genomes.get(**wildcards).output[0]
    cluster_ids = get_cluster_ids_from_csv(f"{DREP_DIR}/data_tables/Cdb.csv")
    return f"{MMSEQS_DIR}/cluster_{wildcards.cluster_id}.tsv"

## These functions return a list of all desired files, and can be called directly in the input

def get_gene_fna(wildcards):
    checkpoint_output = checkpoints.cluster_genomes.get(**wildcards).output[0]
    cluster_ids = get_cluster_ids_from_csv(f"{DREP_DIR}/data_tables/Cdb.csv")
    return expand(f"{MMSEQS_DIR}/cluster_{{cluster_id}}.fna", cluster_id=cluster_ids)

def get_gene_faa(wildcards):
    checkpoint_output = checkpoints.cluster_genomes.get(**wildcards).output[0]
    cluster_ids = get_cluster_ids_from_csv(f"{DREP_DIR}/data_tables/Cdb.csv")
    return expand(f"{MMSEQS_DIR}/cluster_{{cluster_id}}.faa", cluster_id=cluster_ids)

def get_gene_tsv(wildcards):
    checkpoint_output = checkpoints.cluster_genomes.get(**wildcards).output[0]
    cluster_ids = get_cluster_ids_from_csv(f"{DREP_DIR}/data_tables/Cdb.csv")
    return expand(f"{MMSEQS_DIR}/cluster_{{cluster_id}}.tsv", cluster_id=cluster_ids)

def get_kofams_tsv(wildcards):
    checkpoint_output = checkpoints.cluster_genomes.get(**wildcards).output[0]
    cluster_ids = get_cluster_ids_from_csv(f"{DREP_DIR}/data_tables/Cdb.csv")
    return expand(f"{KOFAMS_DIR}/cluster_{{cluster_id}}.tsv", cluster_id=cluster_ids)

####
#### The rules below are only executed after the checkpoint
####

rule create_pangenome_heatmap:
    input:
        genome=f"{FINAL_DIR}/genome_gene.csv",
        gene=get_gene_tsv
    output:
        f"{FIGURES_DIR}/heatmap_genes.pdf"
    conda:
        "environments/clustering.yml"
    shell:
        """
        python workflow/scripts/figure_pangenome_genes.py {input.genome} {input.gene} {output}
        """

# Dereplicate the genes to obtain a list of reference genes for the cluster
rule run_mmseqs:
    input:
        cluster_genes=lambda wildcards: get_cluster_fna_sep(wildcards)
    output:
        fna=f"{MMSEQS_DIR}/cluster_{{cluster_id}}.fna",
        tsv=f"{MMSEQS_DIR}/cluster_{{cluster_id}}.tsv"
    params:
        tmp=f"{MMSEQS_DIR}/cluster_{{cluster_id}}_tmp"
    conda:
        "environments/clustering.yml"
    shell:
        """
        mmseqs easy-linclust {input.cluster_genes} {MMSEQS_DIR}/cluster_{wildcards.cluster_id}_output {params.tmp} --min-seq-id 0.8 --cov-mode 1 -c 0.8
        rm {MMSEQS_DIR}/cluster_{wildcards.cluster_id}_output_all_seqs.fasta
        mv {MMSEQS_DIR}/cluster_{wildcards.cluster_id}_output_rep_seq.fasta {output.fna}
        mv {MMSEQS_DIR}/cluster_{wildcards.cluster_id}_output_cluster.tsv {output.tsv}
        rm -rf {params.tmp}
        """

# Translate gene nucleotide sequences to amino acid sequences
rule translate_genes:
    input:
        get_gene_fna_sep
    output:
        f"{MMSEQS_DIR}/cluster_{{cluster_id}}.faa"
    shell:
        """
        python workflow/scripts/translate_genes.py {input} {output}
        """

# Run checkm2 to assess properties of the cluster
rule run_checkm2_clusters:
    input:
        get_gene_fna
    output:
        directory=directory(f"{CHECKM2_CLUSTERS_DIR}"),
        report=f"{CHECKM2_CLUSTERS_DIR}/quality_report.tsv"
    params:
        database=f"{CHECKM2_DATABASE}"
    conda:
        "environments/quantification.yml"
    shell:
        """
        checkm2 predict --threads {threads} --input {input} --output-directory {output.directory} --database_path {params.database}
        """

# Run gtdbtk to assign taxonomy to the cluster
rule run_gtdbtk:
    input:
        clusters=get_gene_fna
    output:
        directory(f"{GTDBTK_DIR}")
    params:
        inputdir=f"{CLUSTER_DIR}",
        database=f"{GTDB_DATABASE}",
        tmp=f"{TEMP_DIR}"
    conda:
        "environments/quantification.yml"
    shell:
        """
        export GTDBTK_DATA_PATH={params.database}
        mkdir {output}
        gtdbtk classify_wf \
            --genome_dir {params.inputdir} \
            --extension fna \
            --out_dir {output} \
            --tmpdir {params.tmp} \
            --cpus {threads} \
            --skip_ani_screen
        """

# Cocatenate gene dereplication map files
rule concatenate_clusters:
    input:
        clusters=get_gene_tsv
    output:
        f"{MMSEQS_DIR}/all_clusters.tsv"
    conda:
        "environments/quantification.yml"
    shell:
        """
        python workflow/scripts/concatenate_clusters.py {input} {output}
        """

# Cocatenate dereplicated nucleotide gene sequences
rule concatenate_clusters_nt:
    input:
        clusters=get_gene_fna
    output:
        f"{BOWTIE_DIR}/all_clusters.fna"
    run:
        with open(output[0], "w") as out_file:
            for cluster_file in input.clusters:
                with open(cluster_file, "r") as in_file:
                    out_file.write(in_file.read())

# Cocatenate dereplicated amino acid gene sequences
rule concatenate_clusters_aa:
    input:
        clusters=get_gene_faa
    output:
        f"{KOFAMS_DIR}/all_clusters.faa"
    run:
        with open(output[0], "w") as out_file:
            for cluster_file in input.clusters:
                with open(cluster_file, "r") as in_file:
                    out_file.write(in_file.read())

# Run KEGG annotation of dereplicated genes
rule run_kofams:
    input:
        aa=lambda wildcards: get_gene_faa_sep(wildcards)
    output:
        txt=f"{KOFAMS_DIR}/cluster_{{cluster_id}}.txt",
        tsv=f"{KOFAMS_DIR}/cluster_{{cluster_id}}.tsv"
    params:
        database=f"{KOFAMS_DATABASE}"
    conda:
        "environments/quantification.yml"
    shell:
        """
        hmmscan -o {output.txt} --tblout {output.tsv} -E 1e-5 --noali {params.database} {input.aa}
        """

# Merge KEGG annotations into a single file and select the best annotation per gene
rule merge_kofams:
    input:
        get_kofams_tsv
    output:
        tsv=f"{KOFAMS_DIR}/all_clusters.tsv",
        csv=f"{KOFAMS_DIR}/all_clusters.csv"
    params:
        database=f"{KOFAMS_DATABASE}"
    conda:
        "environments/quantification.yml"
    shell:
        """
        cat {input} > {output.tsv}
        python workflow/scripts/select_ko.py {output.tsv} {output.csv}
        """

rule create_pangenome_heatmap_kegg:
    input:
        genome=f"{FINAL_DIR}/genome_gene.csv",
        gene=get_gene_tsv,
        info=f"{FINAL_DIR}/cluster_info.csv"
    output:
        f"{FIGURES_DIR}/heatmap_kegg.pdf"
    conda:
        "environments/clustering.yml"
    shell:
        """
        python workflow/scripts/figure_pangenome_kegg.py {input.genome} {input.info} {input.gene} {output}
        """

# Build bowtie2 index to map reads against the cluster reference genes
rule build_bowtie_index:
    input:
        f"{BOWTIE_DIR}/all_clusters.fna"
    output:
        index=expand(f"{BOWTIE_DIR}/all_clusters.{{ext}}", ext=["1.bt2", "2.bt2", "3.bt2", "4.bt2", "rev.1.bt2", "rev.2.bt2"]),
        lengths=f"{FINAL_DIR}/gene_lengths.csv"
    conda:
        "environments/quantification.yml"
    shell:
        """
        mkdir -p {BOWTIE_DIR}
        bowtie2-build {input} {BOWTIE_DIR}/all_clusters
        python workflow/scripts/calculate_gene_lengths.py {input} {output.lengths}
        """

# Map reads against the cluster reference genes
rule map_reads:
    input:
        index=expand(f"{BOWTIE_DIR}/all_clusters.{{ext}}", ext=["1.bt2", "2.bt2", "3.bt2", "4.bt2", "rev.1.bt2", "rev.2.bt2"]),
        r1=f"{READS_DIR}/{{sample}}_1.fq.gz",
        r2=f"{READS_DIR}/{{sample}}_2.fq.gz"
    output:
        f"{BOWTIE_DIR}/{{sample}}.bam"
    conda:
        "environments/quantification.yml"
    shell:
        """
        bowtie2 -x {BOWTIE_DIR}/all_clusters -1 {input.r1} -2 {input.r2} | samtools view -bS - | samtools sort -o {output}
        """

# Calculate mapping results into tables
rule quantify_reads:
    input:
        expand(f"{BOWTIE_DIR}/{{sample}}.bam", sample=samples)
    output:
        f"{COVERM_DIR}/all_clusters.tsv"
    conda:
        "environments/quantification.yml"
    shell:
        """
        coverm contig -m count covered_bases -b {input} -t {threads} -o {output}
        """

# Generate the final tables using the information generated throughout the pipeline
rule final_tables:
    input:
        genomes=f"{FINAL_DIR}/genome_gene.csv",
        lengths=f"{FINAL_DIR}/gene_lengths.csv",
        coverm=f"{COVERM_DIR}/all_clusters.tsv",
        clusters=f"{DREP_DIR}/data_tables/Cdb.csv",
        kofams=f"{KOFAMS_DIR}/all_clusters.csv"
    output:
        info=f"{FINAL_DIR}/cluster_info.csv",
        counts=f"{FINAL_DIR}/gene_counts.csv",
        coverage=f"{FINAL_DIR}/gene_coverage.csv"
    conda:
        "environments/quantification.yml"
    shell:
        """
        python workflow/scripts/separate_coverm.py {input.coverm} {output.counts} {output.coverage}
        python workflow/scripts/pangenome_info.py {input.clusters} {input.genomes} {input.lengths} {input.kofams} {output.info}
        """

rule final_figures:
    input:
        info=f"{FINAL_DIR}/cluster_info.csv",
        counts=f"{FINAL_DIR}/gene_counts.csv",
    output:
        f"{FIGURES_DIR}/heatmap_counts.pdf"
    conda:
        "environments/clustering.yml"
    shell:
        """
        python workflow/scripts/figure_pangenome_counts.py {input.info} {input.counts} {output}
        """

rule aggregate_by_cluster:
    input:
        info=f"{FINAL_DIR}/cluster_info.csv",
        counts=f"{FINAL_DIR}/gene_counts.csv",
        coverage=f"{FINAL_DIR}/gene_coverage.csv"
    output:
        counts=f"{FINAL_DIR}/cluster_counts.csv",
        coverage=f"{FINAL_DIR}/cluster_coverage.csv"
    conda:
        "environments/quantification.yml"
    shell:
        """
        python workflow/scripts/aggregate_by_cluster.py {input.info} {input.counts} {output.counts}
        python workflow/scripts/aggregate_by_cluster.py {input.info} {input.coverage} {output.coverage}
        """
